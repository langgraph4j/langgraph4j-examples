spring:
  profiles:
    default: ollama
  main:
    # studio
    web-application-type: servlet
  ai:
    mcp:
      client:
        enabled: true
        name: postgres
        version: 1.0.0
        request-timeout: 30s
        type: SYNC  # or ASYNC for reactive applications
        stdio:
          root-change-notification: false
          connections:
            server1:
              command: docker
              args:
                - run
                - -i
                - --rm
                - mcp/postgres
                - postgresql://admin:bsorrentino@host.docker.internal:5432/mcp_db
              env:
                DEBUG: true

---
# OpenAI Profile
spring:
  profiles:
    activate:
      on-profile: openai
  ai:
    openai:
      api-key: ${OPENAI_API_KEY:your OpenAI api key}
      # You can add other OpenAI specific configurations here if needed
      # chat:
      #   options:
      #     model: gpt-4o-mini
      #     temperature: 0.7

---
# Ollama Profile
spring:
  config:
    activate:
      on-profile: ollama
  ai:
    ollama:
      base-url: http://localhost:11434 # Default Ollama server URL
      chat:
        options:
          model: llama3.1 # Specify your default Ollama model here
          # You can add other Ollama-specific options if needed, for example:
          # temperature: 0.7
          # top-k: 50
#      embedding: # Uncomment and configure if you want to use Ollama for embeddings under this profile
#        options:
#          model: name-of-your-ollama-embedding-model # e.g., nomic-embed-text model: name-of-your-ollama-embedding-model # e.g., nomic-embed-text